{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "771572f6-73ef-49e7-8012-65049fa8be4f",
   "metadata": {},
   "source": [
    "Venyatha Manne\n",
    "\n",
    "# Task#1: Corpus collection and Corpus Descriptive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7318517b-04e7-4f1c-9a37-64c4795fcdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from pprint import pprint\n",
    "import time\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f3fb74-80a0-4001-b624-4668f7f02a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ratemd.25k.all.txt\", sep=\"\\n\", header=None)\n",
    "\n",
    "# make a list that is : Doctor | Gender | Location | Specializations | (list of) Reviews\n",
    "def make_complete_list(data):\n",
    "    data = data.values.tolist()\n",
    "    data_arr = []\n",
    "    curr_reviews = []\n",
    "    last_splitted = []\n",
    "    l = data[0][0].split(\"\\t\")\n",
    "\n",
    "    for line in data[1:]:    \n",
    "        splitted = line[0].split(\"\\t\")\n",
    "        length = len(splitted)\n",
    "        \n",
    "        # if splitted is a doctor header line\n",
    "        if length == 4: \n",
    "            \n",
    "            # ignore doctors with no reviews\n",
    "            if len(curr_reviews) == 0:\n",
    "                continue\n",
    "\n",
    "            l.append(curr_reviews)\n",
    "            data_arr.append(l)\n",
    "            curr_reviews = []\n",
    "            l = splitted\n",
    "\n",
    "            \n",
    "        # if splitted is a review\n",
    "        if length == 2:\n",
    "            # ignore reviews that don't have a written review, regardless of whether they have a rating\n",
    "            if (splitted[1] != \" \"):\n",
    "                curr_reviews.append(splitted)\n",
    "\n",
    "                \n",
    "    # add final entry\n",
    "    l.append(curr_reviews)\n",
    "    data_arr.append(l)\n",
    "    \n",
    "    return data_arr\n",
    "\n",
    "data_list = make_complete_list(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6666a197-cf6e-4f39-9018-d56710a1ad1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female Positive =  2859 , 62.02 %\n",
      "Female Negative =  1751 , 37.98 %\n",
      "Male Positive =  10354 , 68.84 %\n",
      "Male Negative =  4686 , 31.16 %\n",
      "Total Female =  4610 , 23.46 %\n",
      "Total Male =  15040 , 76.54 %\n"
     ]
    }
   ],
   "source": [
    "# a review > 3 is positive, else it is negative\n",
    "def calc_sentiment(review):\n",
    "    value = float(review[0].split(\" \")[2])\n",
    "    if value > 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# prints out the counts and percentages of positive and negative reviews categorized by gender\n",
    "# returns a sentiment list corresponding to every review (will be used to determine collections for ccLDA)\n",
    "def count_sentiment(data_list):\n",
    "    f_pos = 0\n",
    "    f_neg = 0\n",
    "    m_pos = 0\n",
    "    m_neg = 0\n",
    "    \n",
    "    # updated for use in CCLDA for part 2\n",
    "    sentiment_list = [] # 1 for pos, 0 for neg\n",
    "    gender_list = [] # 1 for female, 0 for neg\n",
    "\n",
    "    for item in data_list:        \n",
    "        if item[1].strip().lower() == \"female\":\n",
    "            for i in range(len(item[4])):\n",
    "                sentiment = calc_sentiment(item[4][i])\n",
    "                sentiment_list.append(sentiment)\n",
    "                gender_list.append(1)\n",
    "                if sentiment == 1:\n",
    "                    f_pos += 1\n",
    "                else:\n",
    "                    f_neg += 1\n",
    "\n",
    "        elif item[1].strip().lower() == \"male\":\n",
    "            for i in range(len(item[4])):\n",
    "                sentiment = calc_sentiment(item[4][i])\n",
    "                sentiment_list.append(sentiment)\n",
    "                gender_list.append(0)\n",
    "                if sentiment == 1:\n",
    "                    m_pos += 1\n",
    "                else:\n",
    "                    m_neg += 1\n",
    "            \n",
    "    # calculate percentages:\n",
    "    total = f_pos + f_neg + m_pos + m_neg\n",
    "    f_pos_p =  (f_pos / (f_pos + f_neg))*100\n",
    "    m_pos_p =  (m_pos /  (m_pos + m_neg))*100\n",
    "    f_neg_p = (f_neg / (f_pos + f_neg))*100\n",
    "    m_neg_p = (m_neg / (m_pos + m_neg))*100\n",
    "    \n",
    "    f_total = \"{:.2f}\".format(((f_pos+f_neg)/total)*100)\n",
    "    m_total = \"{:.2f}\".format(((m_pos+m_neg)/total)*100)\n",
    "    \n",
    "    \n",
    "    # print out values for table        \n",
    "    print(\"Female Positive = \", f_pos, \",\", \"{:.2f}\".format(f_pos_p),\"%\")\n",
    "    print(\"Female Negative = \", f_neg, \",\",\"{:.2f}\".format(f_neg_p),\"%\")\n",
    "    print(\"Male Positive = \", m_pos, \",\",\"{:.2f}\".format(m_pos_p),\"%\")\n",
    "    print(\"Male Negative = \", m_neg, \",\",\"{:.2f}\".format(m_neg_p ),\"%\")\n",
    "    \n",
    "    print(\"Total Female = \", f_pos+f_neg,\",\", f_total,\"%\")    \n",
    "    print(\"Total Male = \", m_pos+m_neg,\",\", m_total,\"%\")\n",
    "    return sentiment_list, gender_list\n",
    "    \n",
    "# used in Task 2b\n",
    "sentiment_list, gender_list = count_sentiment(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c7ca20-1557-47c9-b80b-9f5413c77bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest review: 899\n",
      "Shortest review: 1\n",
      "Average length: 65\n"
     ]
    }
   ],
   "source": [
    "# prints the length of the shortest and longest review as well as the average length of a review\n",
    "# A review is defined as the number of tokens (i.e., any sequence of characters separated by space and/or beginning/end of review). \n",
    "def get_review_lengths():\n",
    "    all_reviews = []\n",
    "    \n",
    "    \n",
    "    for item in data_list:\n",
    "        reviews_list = item[4]\n",
    "        \n",
    "        for i in range(len(reviews_list)):\n",
    "            review = reviews_list[i][1]\n",
    "            length = len(review.split())\n",
    "            if length > 0:\n",
    "                all_reviews.append(length)\n",
    "    return all_reviews\n",
    "            \n",
    "reviews = get_review_lengths()\n",
    "print(\"Longest review:\", max(reviews))\n",
    "print(\"Shortest review:\", min(reviews))\n",
    "print(\"Average length:\", \"{:.0f}\".format((sum(reviews)/len(reviews))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be1c2b7-9fb9-42fa-8f7f-5180a3188907",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011878a0-b8c5-4254-8f82-7f7502d0d598",
   "metadata": {},
   "source": [
    "# Task#2a: Exploratory Analysis of Corpus with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d7f4d02-d6d8-4a42-ad41-715c35dbc3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mannev1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97604c4e-c475-4b5e-9cdd-0432d6f27f30",
   "metadata": {},
   "source": [
    "## Problem#1: Without lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c711638b-875c-441d-96aa-022665cc92a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of dictionary:  7972\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Clean Corpus\n",
    "# make list of all the reviews     \n",
    "# clean the text: punctuation, lowercase, stopwords\n",
    "def clean_text(speech):\n",
    "    text = speech\n",
    "    text = text.lower()\n",
    "    text = re.sub('—\\[.*?\\]', '', text)\n",
    "    text = re.sub('—', ' ', text)\n",
    "    text = re.sub('“', '', text)\n",
    "    text = re.sub('”', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('[\\d\\n]', ' ', text)\n",
    "    \n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopwords_ = set(stopwords.words('english'))\n",
    "    text = [word.lower() for word in word_tokens\n",
    "             if len(word) > 2 \n",
    "             and word not in stopwords_]\n",
    "    return text   \n",
    "\n",
    "all_reviews = []\n",
    "for item in data_list:\n",
    "    review_list = item[4]\n",
    "    for review in review_list:\n",
    "        r = review[1]\n",
    "        r = \"\".join(r)\n",
    "        all_reviews.append(clean_text(r))\n",
    "\n",
    "# Step 2: Create the dictionary\n",
    "id2word = corpora.Dictionary(all_reviews)\n",
    "\n",
    "# further preprocessing: have occurred in less than 4 articles or have occurred in more than 40% of the articles\n",
    "id2word.filter_extremes(no_below=4, no_above=0.4)\n",
    "term_dictionary = id2word.token2id\n",
    "print(\"size of dictionary: \", len(term_dictionary))\n",
    "\n",
    "\n",
    "# Step 3: Convert the list of documents in your corpus into Document-Term Matrix\n",
    "doc_term_matrix = [id2word.doc2bow(text) for text in all_reviews]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7c9626f-cc1f-4a7a-a0d9-33e500bda533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 1, Set 1 --- 130.4043300151825 seconds ---\n",
      "Problem 1, Set 2 --- 132.97442770004272 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Run the LDA model on the document-term matrix\n",
    "# Set 1: number of topics (k = 10), number of passes (pass = 20), and number of iterations (iterations = 2000).\n",
    "start_time = time.time()\n",
    "lda_model = gensim.models.LdaMulticore(corpus=doc_term_matrix, id2word=id2word, num_topics=10, passes=20, iterations=2000)\n",
    "print(\"Problem 1, Set 1\",\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Set 2: number of topics (k = 20), number of passes (pass = 20), and number of iterations (iterations = 2000).\n",
    "start_time = time.time()\n",
    "lda_model2 = gensim.models.LdaMulticore(corpus=doc_term_matrix, id2word=id2word, num_topics=20, passes=20, iterations=2000)\n",
    "print(\"Problem 1, Set 2\",\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "335986f5-80ca-4849-bdf3-5a778bd40fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 1, Set 1:\n",
      "+----+-----------+-----------+-----------+-------------+-------------+\n",
      "|    | Topic 1   | Topic 2   | Topic 3   | Topic 4     | Topic 5     |\n",
      "|----+-----------+-----------+-----------+-------------+-------------|\n",
      "|  0 | doctor    | doctor    | doctor    | office      | time        |\n",
      "|  1 | patients  | time      | office    | staff       | room        |\n",
      "|  2 | rude      | great     | told      | doctor      | doctor      |\n",
      "|  3 | like      | best      | insurance | time        | see         |\n",
      "|  4 | ever      | recommend | would     | wait        | appointment |\n",
      "|  5 | care      | always    | said      | get         | minutes     |\n",
      "|  6 | never     | staff     | called    | always      | waiting     |\n",
      "|  7 | would     | feel      | get       | call        | wait        |\n",
      "|  8 | patient   | would     | asked     | never       | went        |\n",
      "|  9 | one       | questions | never     | appointment | never       |\n",
      "+----+-----------+-----------+-----------+-------------+-------------+\n",
      "+----+-----------+---------------+-----------+-----------+------------+\n",
      "|    | Topic 6   | Topic 7       | Topic 8   | Topic 9   | Topic 10   |\n",
      "|----+-----------+---------------+-----------+-----------+------------|\n",
      "|  0 | pain      | doctor        | surgery   | surgery   | staff      |\n",
      "|  1 | would     | caring        | surgeon   | went      | recommend  |\n",
      "|  2 | years     | excellent     | staff     | pain      | would      |\n",
      "|  3 | back      | years         | recommend | son       | years      |\n",
      "|  4 | doctor    | patient       | procedure | told      | schwartz   |\n",
      "|  5 | hospital  | knowledgeable | would     | would     | helpful    |\n",
      "|  6 | life      | best          | great     | said      | treatment  |\n",
      "|  7 | told      | staff         | results   | first     | brown      |\n",
      "|  8 | patient   | care          | breast    | baby      | highly     |\n",
      "|  9 | one       | good          | highly    | husband   | experience |\n",
      "+----+-----------+---------------+-----------+-----------+------------+\n",
      "Problem 1, Set 2:\n",
      "+----+-------------+-----------+-----------+-----------+-----------+\n",
      "|    | Topic 1     | Topic 2   | Topic 3   | Topic 4   | Topic 5   |\n",
      "|----+-------------+-----------+-----------+-----------+-----------|\n",
      "|  0 | doctor      | time      | told      | like      | care      |\n",
      "|  1 | appointment | office    | would     | would     | patients  |\n",
      "|  2 | office      | wait      | said      | said      | doctor    |\n",
      "|  3 | time        | staff     | doctor    | told      | always    |\n",
      "|  4 | room        | long      | back      | went      | family    |\n",
      "|  5 | see         | get       | called    | get       | staff     |\n",
      "|  6 | never       | like      | never     | doctor    | see       |\n",
      "|  7 | get         | know      | call      | wrong     | patient   |\n",
      "|  8 | told        | doctor    | get       | never     | years     |\n",
      "|  9 | staff       | one       | first     | even      | get       |\n",
      "+----+-------------+-----------+-----------+-----------+-----------+\n",
      "+----+-----------+---------------+-----------+-----------+------------+\n",
      "|    | Topic 6   | Topic 7       | Topic 8   | Topic 9   | Topic 10   |\n",
      "|----+-----------+---------------+-----------+-----------+------------|\n",
      "|  0 | staff     | staff         | time      | best      | pain       |\n",
      "|  1 | doctor    | great         | questions | doctor    | back       |\n",
      "|  2 | rude      | doctor        | takes     | ever      | would      |\n",
      "|  3 | office    | helpful       | always    | years     | told       |\n",
      "|  4 | worst     | recommend     | doctor    | one       | went       |\n",
      "|  5 | would     | knowledgeable | patient   | doctors   | doctor     |\n",
      "|  6 | never     | friendly      | concerns  | seen      | saw        |\n",
      "|  7 | patients  | would         | patients  | family    | get        |\n",
      "|  8 | ever      | good          | caring    | many      | mri        |\n",
      "|  9 | care      | highly        | answer    | caring    | months     |\n",
      "+----+-----------+---------------+-----------+-----------+------------+\n",
      "+----+------------+---------------+-------------+------------+--------------+\n",
      "|    | Topic 11   | Topic 12      | Topic 13    | Topic 14   | Topic 15     |\n",
      "|----+------------+---------------+-------------+------------+--------------|\n",
      "|  0 | treatment  | excellent     | feel        | insurance  | doctor       |\n",
      "|  1 | surgery    | highly        | made        | office     | patient      |\n",
      "|  2 | problem    | surgeon       | comfortable | staff      | patients     |\n",
      "|  3 | life       | recommend     | baby        | pay        | years        |\n",
      "|  4 | shakiba    | surgery       | daughter    | dentist    | also         |\n",
      "|  5 | years      | caring        | pregnancy   | procedure  | good         |\n",
      "|  6 | pain       | compassionate | always      | bill       | one          |\n",
      "|  7 | medical    | care          | children    | company    | need         |\n",
      "|  8 | would      | patel         | first       | work       | psychiatrist |\n",
      "|  9 | well       | would         | delivered   | would      | treatment    |\n",
      "+----+------------+---------------+-------------+------------+--------------+\n",
      "+----+-------------+------------+------------+------------+------------+\n",
      "|    | Topic 16    | Topic 17   | Topic 18   | Topic 19   | Topic 20   |\n",
      "|----+-------------+------------+------------+------------+------------|\n",
      "|  0 | son         | manner     | surgery    | symptoms   | life       |\n",
      "|  1 | knee        | bedside    | went       | medical    | thank      |\n",
      "|  2 | years       | doctor     | would      | diagnosed  | cancer     |\n",
      "|  3 | old         | patient    | surgeon    | disease    | god        |\n",
      "|  4 | year        | poor       | procedure  | problem    | staff      |\n",
      "|  5 | surgery     | opinion    | schwartz   | even       | saved      |\n",
      "|  6 | replacement | medication | removed    | daughter   | husband    |\n",
      "|  7 | hip         | another    | skin       | patient    | breast     |\n",
      "|  8 | ago         | child      | eye        | brain      | done       |\n",
      "|  9 | two         | second     | results    | aviv       | brown      |\n",
      "+----+-------------+------------+------------+------------+------------+\n"
     ]
    }
   ],
   "source": [
    "def print_tables(topics, title, num_topics):\n",
    "    table = []\n",
    "    for topic in topics:\n",
    "        word_list = []\n",
    "        for word in topic[0]:\n",
    "            word_list.append(word[1])\n",
    "        table.append(word_list)\n",
    "    \n",
    "    print(title)\n",
    "    dict1 = {'Topic 1': table[0], 'Topic 2': table[1], 'Topic 3': table[2], 'Topic 4': table[3], 'Topic 5': table[4]}\n",
    "    df1 = pd.DataFrame(dict1)\n",
    "    print(tabulate(df1, headers = 'keys', tablefmt = 'psql'))\n",
    "    \n",
    "    dict2 = {'Topic 6': table[5], 'Topic 7': table[6], 'Topic 8': table[7], 'Topic 9': table[8], 'Topic 10': table[9]}\n",
    "    df2 = pd.DataFrame(dict2)\n",
    "    print(tabulate(df2, headers = 'keys', tablefmt = 'psql'))\n",
    "    \n",
    "    if (num_topics == 20):\n",
    "        dict1 = {'Topic 11': table[10], 'Topic 12': table[11], 'Topic 13': table[12], 'Topic 14': table[13], 'Topic 15': table[14]}\n",
    "        df1 = pd.DataFrame(dict1)\n",
    "        print(tabulate(df1, headers = 'keys', tablefmt = 'psql'))\n",
    "\n",
    "        dict2 = {'Topic 16': table[15], 'Topic 17': table[16], 'Topic 18': table[17], 'Topic 19': table[18], 'Topic 20': table[19]}\n",
    "        df2 = pd.DataFrame(dict2)\n",
    "        print(tabulate(df2, headers = 'keys', tablefmt = 'psql'))\n",
    "        \n",
    "\n",
    "top_topics = lda_model.top_topics(doc_term_matrix, topn=10)\n",
    "print_tables(top_topics, \"Problem 1, Set 1:\", 10)\n",
    "top_topics2 = lda_model2.top_topics(doc_term_matrix, topn=10)\n",
    "print_tables(top_topics2, \"Problem 1, Set 2:\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5002473-e048-4e07-8fe4-e5760704c30f",
   "metadata": {},
   "source": [
    "## Problem#2: With lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00b7143e-f804-445a-9b1a-cbffd04a14f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "405fa4e4-bb11-4d37-84b6-42af33441741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to lemmatize: --- 125.15052008628845 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# # Lemmatize\n",
    "\n",
    "# https://www.machinelearningplus.com/nlp/lemmatization-examples-python/#wordnetlemmatizer\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "start_time = time.time()\n",
    "lemmatized_reviews = []\n",
    "for review in all_reviews:\n",
    "    sentence = \" \".join(review)\n",
    "    lemmatized_reviews.append([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])\n",
    "lemma_time = time.time() - start_time\n",
    "print(\"Time taken to lemmatize:\",\"--- %s seconds ---\" % (lemma_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f1c1030-2c90-442a-956b-b96ad4d8dc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of dictionary:  7972\n",
      "Problem 1, Set 1 --- 141.8912582397461 seconds ---\n",
      "Time with lemmatization 267.04177832603455\n",
      "\n",
      "\n",
      "Problem 1, Set 2 --- 133.83016920089722 seconds ---\n",
      "Time with lemmatization 258.98068928718567\n",
      "Problem 1, Set 1:\n",
      "+----+-----------+-----------+-------------+-----------+-----------+\n",
      "|    | Topic 1   | Topic 2   | Topic 3     | Topic 4   | Topic 5   |\n",
      "|----+-----------+-----------+-------------+-----------+-----------|\n",
      "|  0 | call      | pain      | wait        | time      | office    |\n",
      "|  1 | get       | go        | time        | child     | staff     |\n",
      "|  2 | say       | say       | appointment | take      | rude      |\n",
      "|  3 | would     | would     | see         | like      | patient   |\n",
      "|  4 | told      | told      | hour        | feel      | call      |\n",
      "|  5 | back      | get       | get         | son       | insurance |\n",
      "|  6 | day       | want      | minute      | care      | get       |\n",
      "|  7 | see       | back      | room        | see       | bad       |\n",
      "|  8 | go        | see       | go          | make      | never     |\n",
      "|  9 | never     | even      | take        | always    | like      |\n",
      "+----+-----------+-----------+-------------+-----------+-----------+\n",
      "+----+-----------+-----------+-----------+-----------+------------+\n",
      "|    | Topic 6   | Topic 7   | Topic 8   | Topic 9   | Topic 10   |\n",
      "|----+-----------+-----------+-----------+-----------+------------|\n",
      "|  0 | staff     | patient   | care      | surgery   | life       |\n",
      "|  1 | recommend | care      | best      | surgeon   | save       |\n",
      "|  2 | great     | time      | recommend | procedure | would      |\n",
      "|  3 | time      | year      | great     | pain      | breast     |\n",
      "|  4 | question  | take      | patient   | perform   | make       |\n",
      "|  5 | office    | need      | would     | year      | patel      |\n",
      "|  6 | helpful   | always    | year      | go        | year       |\n",
      "|  7 | make      | treatment | excellent | result    | go         |\n",
      "|  8 | answer    | see       | highly    | remove    | one        |\n",
      "|  9 | friendly  | health    | manner    | cancer    | family     |\n",
      "+----+-----------+-----------+-----------+-----------+------------+\n",
      "Problem 1, Set 2:\n",
      "+----+-------------+-----------+-----------+-----------+-----------+\n",
      "|    | Topic 1     | Topic 2   | Topic 3   | Topic 4   | Topic 5   |\n",
      "|----+-------------+-----------+-----------+-----------+-----------|\n",
      "|  0 | wait        | say       | go        | recommend | life      |\n",
      "|  1 | time        | told      | say       | time      | ever      |\n",
      "|  2 | appointment | ask       | get       | question  | treat     |\n",
      "|  3 | see         | go        | would     | staff     | save      |\n",
      "|  4 | hour        | never     | told      | answer    | child     |\n",
      "|  5 | get         | bad       | take      | take      | year      |\n",
      "|  6 | office      | see       | time      | great     | know      |\n",
      "|  7 | minute      | would     | back      | would     | family    |\n",
      "|  8 | patient     | back      | day       | care      | care      |\n",
      "|  9 | staff       | like      | visit     | helpful   | patient   |\n",
      "+----+-------------+-----------+-----------+-----------+-----------+\n",
      "+----+-------------+-----------+-----------+-----------+------------+\n",
      "|    | Topic 6     | Topic 7   | Topic 8   | Topic 9   | Topic 10   |\n",
      "|----+-------------+-----------+-----------+-----------+------------|\n",
      "|  0 | feel        | care      | best      | call      | surgery    |\n",
      "|  1 | make        | excellent | manner    | office    | perform    |\n",
      "|  2 | like        | recommend | bedside   | get       | problem    |\n",
      "|  3 | comfortable | patient   | ever      | patient   | surgeon    |\n",
      "|  4 | great       | year      | great     | phone     | pain       |\n",
      "|  5 | care        | family    | good      | never     | back       |\n",
      "|  6 | patient     | staff     | one       | would     | remove     |\n",
      "|  7 | felt        | highly    | care      | test      | well       |\n",
      "|  8 | seem        | would     | see       | return    | result     |\n",
      "|  9 | staff       | physician | love      | ask       | recommend  |\n",
      "+----+-------------+-----------+-----------+-----------+------------+\n",
      "+----+------------+------------+------------+------------+------------+\n",
      "|    | Topic 11   | Topic 12   | Topic 13   | Topic 14   | Topic 15   |\n",
      "|----+------------+------------+------------+------------+------------|\n",
      "|  0 | patient    | staff      | patient    | baby       | son        |\n",
      "|  1 | care       | office     | care       | pregnancy  | eye        |\n",
      "|  2 | time       | rude       | time       | first      | mother     |\n",
      "|  3 | health     | front      | problem    | deliver    | mom        |\n",
      "|  4 | staff      | would      | treatment  | make       | old        |\n",
      "|  5 | need       | like       | take       | go         | give       |\n",
      "|  6 | well       | work       | listen     | look       | child      |\n",
      "|  7 | issue      | nurse      | diagnosis  | breast     | see        |\n",
      "|  8 | know       | well       | see        | amaze      | year       |\n",
      "|  9 | concern    | people     | treat      | would      | use        |\n",
      "+----+------------+------------+------------+------------+------------+\n",
      "+----+------------+------------+------------+---------------+------------+\n",
      "|    | Topic 16   | Topic 17   | Topic 18   | Topic 19      | Topic 20   |\n",
      "|----+------------+------------+------------+---------------+------------|\n",
      "|  0 | insurance  | pain       | year       | treatment     | surgery    |\n",
      "|  1 | office     | year       | see        | patient       | surgeon    |\n",
      "|  2 | pay        | go         | would      | skill         | cancer     |\n",
      "|  3 | money      | test       | know       | thank         | another    |\n",
      "|  4 | bill       | give       | old        | lack          | ray        |\n",
      "|  5 | rude       | would      | daughter   | competent     | teeth      |\n",
      "|  6 | company    | problem    | practice   | compassion    | go         |\n",
      "|  7 | charge     | month      | move       | god           | tooth      |\n",
      "|  8 | staff      | knee       | man        | knowledge     | experience |\n",
      "|  9 | horrible   | medication | good       | communication | nose       |\n",
      "+----+------------+------------+------------+---------------+------------+\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create the dictionary\n",
    "id2word_l = corpora.Dictionary(lemmatized_reviews)\n",
    "\n",
    "# further preprocessing: have occurred in less than 4 articles or have occurred in more than 40% of the articles\n",
    "id2word_l.filter_extremes(no_below=4, no_above=0.4)\n",
    "term_dictionary_l = id2word.token2id\n",
    "print(\"size of dictionary: \", len(term_dictionary_l))\n",
    "\n",
    "\n",
    "# Step 3: Convert the list of documents in your corpus into Document-Term Matrix\n",
    "doc_term_matrix_l = [id2word_l.doc2bow(text) for text in lemmatized_reviews]\n",
    "\n",
    "# Step 4: Run the LDA model on the document-term matrix\n",
    "# Set 1: number of topics (k = 10), number of passes (pass = 20), and number of iterations (iterations = 2000).\n",
    "start_time = time.time()\n",
    "lda_model_l = gensim.models.LdaMulticore(corpus=doc_term_matrix_l, id2word=id2word_l, num_topics=10, passes=20, iterations=2000)\n",
    "ldatime = time.time() - start_time\n",
    "print(\"Problem 1, Set 1\",\"--- %s seconds ---\" % (ldatime))\n",
    "print(\"Time with lemmatization %s\" % (ldatime+lemma_time))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Set 2: number of topics (k = 20), number of passes (pass = 20), and number of iterations (iterations = 2000).\n",
    "start_time = time.time()\n",
    "lda_model2_l = gensim.models.LdaMulticore(corpus=doc_term_matrix_l, id2word=id2word_l, num_topics=20, passes=20, iterations=2000)\n",
    "ldatime = time.time() - start_time\n",
    "print(\"Problem 1, Set 2\",\"--- %s seconds ---\" % (ldatime))\n",
    "print(\"Time with lemmatization %s\" % (ldatime+lemma_time))\n",
    "\n",
    "\n",
    "top_topics_l = lda_model_l.top_topics(doc_term_matrix_l, topn=10)\n",
    "print_tables(top_topics_l, \"Problem 1, Set 1:\",10)\n",
    "top_topics2_l = lda_model2_l.top_topics(doc_term_matrix_l, topn=10)\n",
    "print_tables(top_topics2_l, \"Problem 1, Set 2:\",20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc29c7d8-8ef2-479f-94a3-a0fee78c3255",
   "metadata": {},
   "source": [
    "# Task#2b: Exploratory Analysis of Corpus with ccLDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4da56b5a-c9c3-48cd-a013-079bf1d7ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cclda data preperation\n",
    "# split into collections and write to a txt file\n",
    "# 0 = f_pos, 1 = f_neg, 2 = m_pos, 3 = m_neg\n",
    "\n",
    "reviews_ = lemmatized_reviews # switch out lemmatized_reviews for all_reviews if you want to run without lemmatization\n",
    "\n",
    "file = open(\"input_file.txt\",\"w\")\n",
    "\n",
    "for i in range(len(sentiment_list)):    \n",
    "    if gender_list[i] == 1: # female\n",
    "        if sentiment_list[i] == 1:\n",
    "            file.write(\"0 \" + \" \".join(reviews_[i]) + \"\\n\") #f_pos\n",
    "        else:\n",
    "            file.write(\"1 \" + \" \".join(reviews_[i]) + \"\\n\")\n",
    "    else:\n",
    "        if sentiment_list[i] == 1:\n",
    "            file.write(\"2 \" + \" \".join(reviews_[i]) + \"\\n\") #f_pos\n",
    "        else:\n",
    "            file.write(\"3 \" + \" \".join(reviews_[i]) + \"\\n\")\n",
    "file.close()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f52d8b-c4cf-4112-b60a-344ab4b8c120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
